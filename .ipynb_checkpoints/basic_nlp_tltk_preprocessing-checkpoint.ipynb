{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb1945d",
   "metadata": {},
   "source": [
    "### Applying Basic NLP Python Libraries to Preprocess Text\n",
    "S. Michael, \n",
    "8.30.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592ef65",
   "metadata": {},
   "source": [
    "This notebook uses the NLTK Toolkit in Python to perform basic preproccessing of text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d924d",
   "metadata": {},
   "source": [
    "#### Pre-preprocessing\n",
    "1. Check Python version\n",
    "2. Load Gutenberg Project in nltk package & List available texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4796c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 18:29:29) \n",
      "[Clang 12.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "#Check Python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46a65be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Gutenberg Project & Select Text\n",
    "#source: https://www.nltk.org/book/ch02.html\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662fd69",
   "metadata": {},
   "source": [
    "#### Pre-processing Using NLTK Toolkit\n",
    "1. Load text: Leaves of Grass by Walt Whitman (Note: loaded tokenized by words)\n",
    "2. Convert to lower case\n",
    "3. Remove punctuation\n",
    "4. Remove stopwords\n",
    "5. Stem\n",
    "6. Lemmatize\n",
    "7. Correct misspellings\n",
    "8. Find 10 most used words\n",
    "9. Plot 10 most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5971d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load text tokenized by words: Leaves of Grass by Walt Whitman\n",
    "raw_text = gutenberg.words('whitman-leaves.txt')\n",
    "#len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f133a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower case\n",
    "def lower_case(text):\n",
    "    return [w.lower() for w in text]\n",
    "text=lower_case(raw_text)\n",
    "#text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "import string\n",
    "def remove_punc(text):\n",
    "    no_punc = [w.translate(str.maketrans('', '', string.punctuation)) for w in text]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    return [word for word in no_punc if word.isalpha()]\n",
    "text = remove_punc(text)\n",
    "#text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if not word in stop_words]\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "#text[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#instantiatte stemming object\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    return [porter_stemmer.stem(word) for word in text]\n",
    "stemmed_text = stemming(text)\n",
    "stemmed_text[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize\n",
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "#instantiate Lemmatization object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "lemm_text = lemmatizing(text)\n",
    "lemm_text[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct misspellings\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "correct_words = words.words()\n",
    "def spelling_correction(text):\n",
    "    for word in text:\n",
    "        temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "        print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212aeaac",
   "metadata": {},
   "source": [
    "lemm_text = spelling_correction(lemm_text)\n",
    "lemm_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73bc30",
   "metadata": {},
   "source": [
    "stem_text = spelling_correction(stem_text)\n",
    "stem_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find 10 most used words\n",
    "from collections import Counter\n",
    "counter_obj = Counter(lemm_text)\n",
    "top10=counter_obj.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2594391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 10 most used words\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.bar(range(len(top10)), [val[1] for val in top10], align='center')\n",
    "plt.xticks(range(len(top10)), [val[0] for val in top10])\n",
    "plt.xticks(rotation=70)\n",
    "plt.title('10 Most Frequent Words in Leaves of Grass')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
